{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce6b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\91996\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91996\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\91996\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\91996\\anaconda3\\lib\\site-packages (3.5.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\91996\\anaconda3\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: markdown in c:\\users\\91996\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\91996\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\91996\\anaconda3\\lib\\site-packages (2.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement sqlite3 (from versions: none)\n",
      "ERROR: No matching distribution found for sqlite3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.5.0/en_core_web_md-3.5.0-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 0.0/42.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 5.8/42.8 MB 29.3 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 12.3/42.8 MB 30.8 MB/s eta 0:00:01\n",
      "     ----------------- --------------------- 19.1/42.8 MB 30.9 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 21.2/42.8 MB 30.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 22.3/42.8 MB 22.7 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 24.1/42.8 MB 19.6 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 26.7/42.8 MB 18.6 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 31.2/42.8 MB 19.0 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 34.1/42.8 MB 18.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 39.3/42.8 MB 19.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.7/42.8 MB 19.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.8/42.8 MB 18.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from en-core-web-md==3.5.0) (3.5.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.10.10)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (77.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (22.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91996\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (8.1.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91996\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-md==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 15:20:33.624986: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-23 15:20:37.385665: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies for RSS fetching, NLP processing, and summarization\n",
    "!pip install requests beautifulsoup4 lxml spacy transformers markdown scikit-learn pandas sqlite3\n",
    "!python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d903ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91996\\AppData\\Local\\Temp\\ipykernel_17832\\3043698543.py:10: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, Markdown, HTML\n"
     ]
    }
   ],
   "source": [
    "### STEP 2: Import Necessary Libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "from IPython.core.display import display, Markdown, HTML\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fd96db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\91996\\anaconda3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40e7905d0b74728beca06c5535e7d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91996\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\91996\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "### STEP 3: Initialize NLP Models\n",
    "nlp = spacy.load(\"en_core_web_md\")  # Medium model with word vectors\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad43fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 4: Setup User Preferences Database (SQLite)\n",
    "DB_FILE = \"flashfeed_users.db\"\n",
    "def setup_database():\n",
    "    \"\"\"Creates a user preferences database if not exists.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS user_preferences (\n",
    "                        user TEXT PRIMARY KEY, \n",
    "                        interests TEXT, \n",
    "                        last_read_articles TEXT)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "setup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20ea92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 5: Define User Interest Learning Function\n",
    "def update_user_interests(user, new_interests):\n",
    "    \"\"\"Updates user preferences in the database dynamically.\"\"\"\n",
    "    conn = sqlite3.connect(DB_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT interests FROM user_preferences WHERE user=?\", (user,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        existing_interests = set(json.loads(result[0]))\n",
    "        updated_interests = list(existing_interests.union(set(new_interests)))\n",
    "        cursor.execute(\"UPDATE user_preferences SET interests=? WHERE user=?\", (json.dumps(updated_interests), user))\n",
    "    else:\n",
    "        cursor.execute(\"INSERT INTO user_preferences (user, interests) VALUES (?, ?)\", (user, json.dumps(new_interests)))\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f0d4e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rss_articles(rss_urls):\n",
    "    \"\"\"Fetches and parses news articles from RSS feeds without printing warnings.\"\"\"\n",
    "    articles = []\n",
    "    for url in rss_urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code != 200:\n",
    "                continue  # Silently skip broken RSS feeds\n",
    "\n",
    "            soup = BeautifulSoup(response.content, \"xml\")\n",
    "            for item in soup.find_all(\"item\")[:5]:  # Fetch first 5 articles per feed\n",
    "                title = item.title.text if item.title else \"No Title\"\n",
    "                link = item.link.text if item.link else \"No Link\"\n",
    "                summary = item.description.text if item.description else \"No Summary\"\n",
    "                published = item.pubDate.text if item.pubDate else \"No Date\"\n",
    "\n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"summary\": summary,\n",
    "                    \"published\": published\n",
    "                })\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass  # Suppress error messages\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c67fdf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    \"\"\"Extracts important keywords using NLP and adds more nouns for better filtering.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    keywords = set()\n",
    "\n",
    "    # Extract Named Entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"ORG\", \"PRODUCT\", \"GPE\", \"WORK_OF_ART\", \"TECHNOLOGY\"]:\n",
    "            keywords.add(ent.text.lower())\n",
    "\n",
    "    # Extract Key Nouns, Proper Nouns, and Common Tech Words\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
    "            keywords.add(token.text.lower())\n",
    "\n",
    "    # Include some common tech-related words manually\n",
    "    common_tech_terms = {\"ai\", \"machine learning\", \"deep learning\", \"cybersecurity\", \"blockchain\", \"startup\"}\n",
    "    keywords.update(common_tech_terms)\n",
    "\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e1f07e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relevant_articles(articles, user_interests):\n",
    "    \"\"\"Filters articles to ensure they match user interests based on improved NLP categorization.\"\"\"\n",
    "    relevant_articles = []\n",
    "\n",
    "    for article in articles:\n",
    "        article_text = (article[\"title\"] + \" \" + article[\"summary\"]).lower()\n",
    "        category = categorize_article(article_text)\n",
    "\n",
    "        # Allow articles if at least one keyword or category matches\n",
    "        match_count = sum(1 for interest in user_interests if interest.lower() in category.lower() or interest.lower() in article_text)\n",
    "\n",
    "        if match_count >= 1:  # Loosen the filtering rule\n",
    "            relevant_articles.append(article)\n",
    "    \n",
    "    return relevant_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cbaa0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 7: Categorize Articles Using NLP\n",
    "def categorize_article(article_text):\n",
    "    \"\"\"Categorizes an article based on its semantic meaning using word vector similarity.\"\"\"\n",
    "    categories = {\n",
    "        \"Technology\": \"AI cybersecurity blockchain startups programming machine learning software gadgets data science robotics cloud computing chip manufacturing open-source innovation\",\n",
    "        \"Finance\": \"markets fintech cryptocurrency economics trading stocks investments banking financial analysis inflation\",\n",
    "        \"Sports\": \"Football F1 NBA Olympics esports tennis cricket rugby athletics soccer racing World Cup Champions League\",\n",
    "        \"Entertainment\": \"Movies TV shows music books celebrities film industry Hollywood Netflix Bollywood\",\n",
    "        \"Science\": \"Space exploration biotech physics renewable energy genetics NASA climate change quantum computing astronomy\"\n",
    "    }\n",
    "    \n",
    "    doc = nlp(article_text.lower())\n",
    "    highest_score = 0\n",
    "    best_category = \"Uncategorized\"\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        category_doc = nlp(keywords)\n",
    "        similarity = doc.similarity(category_doc)\n",
    "        # Increase threshold for better accuracy\n",
    "        if similarity > highest_score and similarity > 0.5:  \n",
    "            highest_score = similarity\n",
    "            best_category = category\n",
    "    \n",
    "    return best_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6010537",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 8: Summarize Articles Using AI\n",
    "def summarize_articles(articles):\n",
    "    \"\"\"Summarizes articles using NLP-based text summarization.\"\"\"\n",
    "    summarized_articles = []\n",
    "    for article in articles:\n",
    "        try:\n",
    "            short_summary = summarizer(article[\"summary\"], max_length=50, min_length=20, do_sample=False)\n",
    "            article[\"summary\"] = short_summary[0][\"summary_text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error summarizing article: {e}\")\n",
    "        summarized_articles.append(article)\n",
    "    return summarized_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aee4bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STEP 9: Generate Personalized Newsletters\n",
    "def generate_newsletter(user, articles):\n",
    "    \"\"\"Generates a personalized Markdown newsletter.\"\"\"\n",
    "    newsletter_content = f\"# FlashFeed: Personalized Newsletter for {user}\\n\\n\"\n",
    "    newsletter_content += \"## Top Stories\\n\\n\"\n",
    "    for article in articles:\n",
    "        newsletter_content += f\"### {article['title']}\\n\"\n",
    "        newsletter_content += f\"üìÖ {article['published']}\\n\\n\"\n",
    "        newsletter_content += f\"**Summary:** {article['summary']}\\n\\n\"\n",
    "        newsletter_content += f\"[Read more]({article['link']})\\n\\n\"\n",
    "        newsletter_content += \"---\\n\"\n",
    "    file_name = f\"{user.replace(' ', '_').lower()}_flashfeed.md\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(newsletter_content)\n",
    "    return file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3f3e761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 50, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 50, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "Your max_length is set to 50, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì© Newsletter created for Alex Parker: alex_parker_flashfeed.md\n",
      "üì© Newsletter created for Priya Sharma: priya_sharma_flashfeed.md\n",
      "üì© Newsletter created for Marco Rossi: marco_rossi_flashfeed.md\n",
      "üì© Newsletter created for Lisa Thompson: lisa_thompson_flashfeed.md\n",
      "üì© Newsletter created for David Martinez: david_martinez_flashfeed.md\n"
     ]
    }
   ],
   "source": [
    "def generate_newsletters_for_all():\n",
    "    \"\"\"Fetches, filters, and generates newsletters for all users dynamically.\"\"\"\n",
    "    users = {\n",
    "        \"Alex Parker\": {\n",
    "            \"interests\": [\"AI\", \"cybersecurity\", \"blockchain\", \"startups\", \"programming\"],\n",
    "            \"sources\": [\"https://techcrunch.com/feed/\", \"https://www.wired.com/feed/rss\"]\n",
    "        },\n",
    "        \"Priya Sharma\": {\n",
    "            \"interests\": [\"Global markets\", \"startups\", \"fintech\", \"cryptocurrency\", \"economics\"],\n",
    "            \"sources\": [\"https://www.bloomberg.com/feeds/latest.xml\", \"https://www.forbes.com/finance/feed/\"]\n",
    "        },\n",
    "        \"Marco Rossi\": {\n",
    "            \"interests\": [\"Football\", \"F1\", \"NBA\", \"Olympic sports\", \"esports\"],\n",
    "            \"sources\": [\"https://www.espn.com/espn/rss/news\", \"https://www.skysports.com/rss/12040\"]\n",
    "        },\n",
    "        \"Lisa Thompson\": {\n",
    "            \"interests\": [\"Movies\", \"celebrity news\", \"TV shows\", \"music\", \"books\"],\n",
    "            \"sources\": [\"https://variety.com/feed/\", \"https://www.billboard.com/rss.xml\"]\n",
    "        },\n",
    "        \"David Martinez\": {\n",
    "            \"interests\": [\"Space exploration\", \"AI\", \"biotech\", \"physics\", \"renewable energy\"],\n",
    "            \"sources\": [\"https://www.nasa.gov/rss/dyn/breaking_news.rss\", \"https://www.sciencedaily.com/rss/all.xml\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for user, data in users.items():\n",
    "        articles = fetch_rss_articles(data[\"sources\"])  # Fetch only user-specific sources\n",
    "        filtered_articles = filter_relevant_articles(articles, data[\"interests\"])\n",
    "        summarized_articles = summarize_articles(filtered_articles)\n",
    "        file_name = generate_newsletter(user, summarized_articles)\n",
    "        print(f\"üì© Newsletter created for {user}: {file_name}\")\n",
    "\n",
    "generate_newsletters_for_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f47a5073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91996\\AppData\\Local\\Temp\\ipykernel_17832\\3679544274.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, Markdown\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# FlashFeed: Personalized Newsletter for Alex Parker\n",
       "\n",
       "## Top Stories\n",
       "\n",
       "### Jonah Peretti helped shaped digital media ‚Äî can he do it again?\n",
       "üìÖ Sun, 23 Mar 2025 01:00:52 +0000\n",
       "\n",
       "**Summary:** Jonah Peretti is the founder and CEO of BuzzFeed. Peretti has been at the forefront of digital media for almost two decades. As the company grows older, one question lingers: What is next?\n",
       "\n",
       "[Read more](https://techcrunch.com/2025/03/22/jonah-peretti-helped-shaped-digital-media-can-he-do-it-again/)\n",
       "\n",
       "---\n",
       "### Charlie Javice trial becomes a master class in hubris for both sides\n",
       "üìÖ Sat, 22 Mar 2025 20:49:25 +0000\n",
       "\n",
       "**Summary:** Charlie Javice‚Äôs fraud trial has become a showcase of embarrassing missteps on both sides. JPMorgan Chase was allegedly deceived into buying her startup, Frank, for $175 million when it had just 300,000 customers.\n",
       "\n",
       "[Read more](https://techcrunch.com/2025/03/22/charlie-javice-trial-becomes-a-master-class-in-hubris-for-both-sides/)\n",
       "\n",
       "---\n",
       "### Week in Review: Google buys Wiz¬†\n",
       "üìÖ Sat, 22 Mar 2025 17:05:00 +0000\n",
       "\n",
       "**Summary:** Google made its biggest acquisition in its history this week when it bought Wiz. The NASA astronauts finally came home; Rippling settled its lawsuit.\n",
       "\n",
       "[Read more](https://techcrunch.com/2025/03/22/week-in-review-google-buys-wiz/)\n",
       "\n",
       "---\n",
       "### The 20 hottest open source startups of 2024\n",
       "üìÖ Sat, 22 Mar 2025 14:00:00 +0000\n",
       "\n",
       "**Summary:** A new report showcases the 20 top-trending open source startups around the world. The report is the handiwork of European venture capital firm Runa Capital.\n",
       "\n",
       "[Read more](https://techcrunch.com/2025/03/22/the-20-hottest-open-source-startups-of-2024/)\n",
       "\n",
       "---\n",
       "### Meta settles UK ‚Äòright to object to ad-tracking‚Äô lawsuit by agreeing not to track plaintiff\n",
       "üìÖ Sat, 22 Mar 2025 00:01:00 +0000\n",
       "\n",
       "**Summary:** Human rights campaigner Tanya O&#8217;Carroll has succeeded in forcing social media giant Meta not to use her data for targeted advertising. The agreement is contained in a settlement to an individual challenge she lodged against Meta&#\n",
       "\n",
       "[Read more](https://techcrunch.com/2025/03/21/meta-settles-u-k-right-to-object-to-ad-tracking-lawsuit-by-agreeing-not-to-track-plaintiff/)\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, Markdown\n",
    "\n",
    "def display_newsletter(file_name):\n",
    "    \"\"\"Reads and displays the generated Markdown newsletter inside Jupyter Notebook.\"\"\"\n",
    "    try:\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        display(Markdown(content))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è File not found: {file_name}. Make sure the newsletter was generated successfully.\")\n",
    "\n",
    "# Example usage (Check Alex Parker's feed)\n",
    "display_newsletter(\"alex_parker_flashfeed.md\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
